services:
  metastore:
    image: postgres:11
    hostname: metastore
    environment:
      POSTGRES_PASSWORD: jupyter
      POSTGRES_USER: postgres
      POSTGRES_DB: metastore
    ports:
      - "5433:5432"
    volumes:
      - metastore:/var/lib/postgresql/data
      - ./ddl/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      sparknet:
        ipv4_address: 172.28.1.1
    extra_hosts:
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "worker3:172.28.1.5"
      - "history:172.28.1.6"

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: master
    depends_on:
      - metastore
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
      - ./jupyter/:/home/jupyter/
    networks:
      sparknet:
        ipv4_address: 172.28.1.2
    extra_hosts:
      - "metastore:172.28.1.1"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "worker3:172.28.1.5"
      - "history:172.28.1.6"

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    hostname: worker1
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=2
    ports:
      - "9864:9864"
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      sparknet:
        ipv4_address: 172.28.1.3
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker2:172.28.1.4"
      - "worker3:172.28.1.5"
      - "history:172.28.1.6"

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    hostname: worker2
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=2
    ports:
      - "9865:9864"
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      sparknet:
        ipv4_address: 172.28.1.4
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker3:172.28.1.5"
      - "history:172.28.1.6"

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    hostname: worker3
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=2
    ports:
      - "9866:9864"
    volumes:
      - datanode3:/hadoop/dfs/data
    networks:
      sparknet:
        ipv4_address: 172.28.1.5
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "history:172.28.1.6"

  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=172.28.1.7
      - SPARK_LOCAL_IP=172.28.1.7
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
    ports:
      - "8090:8080"
      - "7078:7077"
      - "4041:4040"
    volumes:
      - ./jupyter/:/home/jupyter/
      - ./spark_jobs:/opt/spark-apps
    networks:
      sparknet:
        ipv4_address: 172.28.1.7

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://172.28.1.7:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
    ports:
      - "8091:8081"
    networks:
      sparknet:
        ipv4_address: 172.28.1.8

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://172.28.1.7:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
    ports:
      - "8092:8081"
    networks:
      sparknet:
        ipv4_address: 172.28.1.9

  spark-worker-3:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-3
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://172.28.1.7:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://master:8020
    ports:
      - "8093:8081"
    networks:
      sparknet:
        ipv4_address: 172.28.1.10

  spark-history:
    image: bde2020/spark-history-server:3.1.1-hadoop3.2
    container_name: spark-history
    hostname: history
    depends_on:
      - spark-master
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=hdfs://master:8020/spark-logs
    ports:
      - "18080:18080"
    volumes:
      - history:/home/jupyter/
    networks:
      sparknet:
        ipv4_address: 172.28.1.6
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "worker3:172.28.1.5"

volumes:
  namenode:
  datanode1:
  datanode2:
  datanode3:
  metastore:
  history:

networks:
  sparknet:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
